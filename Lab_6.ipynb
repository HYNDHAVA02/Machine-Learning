{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5837ee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def summation_unit(inputs, weights):\n",
    "    # Calculates the weighted sum of inputs\n",
    "    return np.dot(inputs, weights)\n",
    "\n",
    "def step_function(x):\n",
    "    # Step activation function\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "def bipolar_step_function(x):\n",
    "    # Bipolar Step activation function\n",
    "    return 1 if x >= 0 else -1\n",
    "\n",
    "def sigmoid_function(x):\n",
    "    # Sigmoid activation function\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh_function(x):\n",
    "    # TanH activation function\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu_function(x):\n",
    "    # ReLU activation function\n",
    "    return max(0, x)\n",
    "\n",
    "def leaky_relu_function(x):\n",
    "    # Leaky ReLU activation function\n",
    "    return x if x > 0 else 0.01 * x\n",
    "\n",
    "def comparator(predictions, targets):\n",
    "    # Error calculation (e.g., Mean Squared Error)\n",
    "    return np.mean((np.array(predictions) - np.array(targets)) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca893b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights: [0.   0.05]\n",
      "Number of epochs: 1000\n",
      "Error list: [3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "def perceptron_training(inputs, targets, weights, learning_rate, activation_function):\n",
    "    epochs = 0\n",
    "    errors = []\n",
    "\n",
    "    while True:\n",
    "        total_error = 0\n",
    "        for i in range(len(inputs)):\n",
    "            weighted_sum = summation_unit(inputs[i], weights)\n",
    "            prediction = activation_function(weighted_sum)\n",
    "            error = targets[i] - prediction\n",
    "            total_error += error ** 2\n",
    "\n",
    "            # Update weights\n",
    "            for j in range(len(weights)):\n",
    "                weights[j] += learning_rate * error * inputs[i][j]\n",
    "\n",
    "        errors.append(total_error)\n",
    "        epochs += 1\n",
    "\n",
    "        if total_error <= 0.002 or epochs >= 1000:\n",
    "            break\n",
    "\n",
    "    return weights, epochs, errors\n",
    "\n",
    "# Example use case for AND gate logic\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "targets = np.array([0, 0, 0, 1])  # AND gate outputs\n",
    "initial_weights = np.array([0.1, 0.2])  # Initial weights\n",
    "learning_rate = 0.05\n",
    "\n",
    "final_weights, num_epochs, error_list = perceptron_training(inputs, targets, initial_weights, learning_rate, step_function)\n",
    "\n",
    "print(\"Final weights:\", final_weights)\n",
    "print(\"Number of epochs:\", num_epochs)\n",
    "print(\"Error list:\", error_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3313c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bipolar Step Activation Function:\n",
      "Final weights: [0.00000000e+00 1.38777878e-17]\n",
      "Number of epochs: 1000\n",
      "Error list: [3, 3, 3, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3]\n",
      "\n",
      "Sigmoid Activation Function:\n",
      "Final weights: [0.01680655 0.01680689]\n",
      "Number of epochs: 1000\n",
      "Error list: [1.020417234421864, 1.019891351441701, 1.0194026177799687, 1.0189483753802298, 1.0185261540713522, 1.0181336589369703, 1.0177687584210597, 1.0174294731452358, 1.0171139654108663, 1.0168205293573447, 1.0165475817467515, 1.0162936533445563, 1.0160573808658544, 1.0158374994568318, 1.0156328356816424, 1.0154423009855604, 1.0152648856061601, 1.0150996529052516, 1.0149457340953951, 1.0148023233359609, 1.0146686731748673, 1.0145440903133536, 1.0144279316723084, 1.0143196007398811, 1.0142185441812483, 1.0141242486925441, 1.0140362380820431, 1.013954070562749, 1.0138773362415265, 1.0138056547908934, 1.0137386732904807, 1.0136760642260474, 1.0136175236347449, 1.0135627693860945, 1.013511539588873, 1.0134635911147758, 1.0134186982303643, 1.0133766513294138, 1.0133372557583207, 1.013300330727764, 1.0132657083042993, 1.0132332324760178, 1.013202758286827, 1.0131741510343073, 1.0131472855264612, 1.0131220453930174, 1.013098322447271, 1.013076016094725, 1.0130550327850893, 1.013035285504432, 1.0130166933045208, 1.0129991808666106, 1.0129826780971356, 1.0129671197529495, 1.0129524450939351, 1.0129385975609655, 1.012925524477345, 1.0129131767720008, 1.0129015087228244, 1.0128904777186727, 1.0128800440386647, 1.01287017064749, 1.012860823005564, 1.0128519688929265, 1.0128435782458858, 1.0128356230054627, 1.0128280769767792, 1.0128209156985777, 1.012814116322143, 1.0128076574989255, 1.0128015192762398, 1.012795683000441, 1.0127901312270393, 1.0127848476372425, 1.0127798169604587, 1.0127750249023286, 1.0127704580778798, 1.0127661039494338, 1.0127619507689243, 1.012757987524298, 1.012754203889712, 1.0127505901792426, 1.0127471373038655, 1.0127438367314547, 1.0127406804495966, 1.0127376609310073, 1.0127347711013686, 1.0127320043094135, 1.0127293542990898, 1.0127268151836615, 1.0127243814216058, 1.0127220477941776, 1.0127198093845198, 1.0127176615582176, 1.0127155999451802, 1.012713620422771, 1.0127117191000836, 1.0127098923032942, 1.0127081365620054, 1.0127064485965134, 1.0127048253059412, 1.0127032637571622, 1.0127017611744735, 1.0127003149299545, 1.0126989225344718, 1.0126975816292803, 1.0126962899781766, 1.012695045460175, 1.0126938460626613, 1.012692689874993, 1.01269157508252, 1.0126904999609905, 1.0126894628713183, 1.0126884622546886, 1.0126874966279744, 1.0126865645794476, 1.0126856647647604, 1.01268479590318, 1.0126839567740618, 1.0126831462135417, 1.0126823631114343, 1.012681606408321, 1.012680875092822, 1.0126801681990292, 1.0126794848040996, 1.012678824025993, 1.0126781850213442, 1.012677566983464, 1.0126769691404598, 1.0126763907534626, 1.012675831114965, 1.0126752895472482, 1.0126747654009063, 1.012674258053453, 1.0126737669080061, 1.0126732913920506, 1.0126728309562676, 1.0126723850734303, 1.0126719532373623, 1.0126715349619508, 1.0126711297802156, 1.0126707372434298, 1.0126703569202835, 1.0126699883960963, 1.0126696312720709, 1.0126692851645847, 1.0126689497045174, 1.012668624536619, 1.0126683093189015, 1.0126680037220708, 1.0126677074289798, 1.0126674201341144, 1.0126671415431014, 1.012666871372243, 1.012666609348073, 1.0126663552069335, 1.0126661086945767, 1.0126658695657786, 1.0126656375839775, 1.0126654125209251, 1.012665194156356, 1.0126649822776732, 1.0126647766796455, 1.0126645771641196, 1.012664383539749, 1.0126641956217273, 1.0126640132315414, 1.01266383619673, 1.0126636643506557, 1.0126634975322852, 1.012663335585982, 1.0126631783613018, 1.0126630257128046, 1.0126628774998694, 1.012662733586516, 1.012662593841239, 1.012662458136845, 1.0126623263502965, 1.0126621983625639, 1.0126620740584824, 1.0126619533266146, 1.0126618360591189, 1.0126617221516232, 1.0126616115031035, 1.0126615040157676, 1.0126613995949416, 1.0126612981489627, 1.0126611995890773, 1.0126611038293385, 1.012661010786511, 1.012660920379981, 1.012660832531664, 1.0126607471659221, 1.01266066420948, 1.0126605835913467, 1.0126605052427382, 1.0126604290970054, 1.0126603550895614, 1.0126602831578153, 1.0126602132411047, 1.0126601452806325, 1.0126600792194067, 1.012660015002181, 1.012659952575396, 1.0126598918871283, 1.012659832887034, 1.0126597755262985, 1.012659719757588, 1.0126596655350015, 1.0126596128140244, 1.0126595615514848, 1.0126595117055102, 1.0126594632354857, 1.0126594161020157, 1.0126593702668836, 1.0126593256930145, 1.0126592823444411, 1.0126592401862666, 1.0126591991846325, 1.0126591593066852, 1.012659120520545, 1.0126590827952762, 1.0126590461008569, 1.0126590104081508, 1.01265897568888, 1.0126589419155985, 1.0126589090616656, 1.0126588771012224, 1.0126588460091666, 1.012658815761131, 1.0126587863334584, 1.0126587577031816, 1.0126587298480019, 1.0126587027462688, 1.0126586763769612, 1.0126586507196653, 1.0126586257545602, 1.0126586014623968, 1.012658577824482, 1.0126585548226619, 1.0126585324393038, 1.0126585106572845, 1.01265848945997, 1.0126584688312046, 1.0126584487552952, 1.0126584292169976, 1.0126584102015022, 1.012658391694423, 1.0126583736817834, 1.0126583561500044, 1.0126583390858936, 1.012658322476633, 1.012658306309767, 1.0126582905731936, 1.0126582752551538, 1.0126582603442191, 1.0126582458292852, 1.01265823169956, 1.0126582179445547, 1.0126582045540768, 1.0126581915182191, 1.012658178827352, 1.0126581664721175, 1.0126581544434174, 1.0126581427324097, 1.0126581313304985, 1.0126581202293277, 1.0126581094207743, 1.0126580988969414, 1.0126580886501517, 1.0126580786729413, 1.0126580689580535, 1.0126580594984325, 1.012658050287218, 1.0126580413177397, 1.012658032583511, 1.0126580240782255, 1.0126580157957497, 1.0126580077301195, 1.0126579998755352, 1.0126579922263559, 1.0126579847770967, 1.0126579775224218, 1.0126579704571432, 1.0126579635762138, 1.0126579568747258, 1.0126579503479045, 1.012657943991106, 1.0126579377998135, 1.012657931769632, 1.0126579258962884, 1.012657920175624, 1.0126579146035941, 1.0126579091762633, 1.0126579038898038, 1.0126578987404906, 1.0126578937247006, 1.0126578888389077, 1.0126578840796825, 1.0126578794436867, 1.0126578749276733, 1.0126578705284832, 1.0126578662430408, 1.012657862068355, 1.0126578580015138, 1.012657854039685, 1.0126578501801111, 1.0126578464201095, 1.012657842757069, 1.0126578391884489, 1.0126578357117761, 1.0126578323246438, 1.01265782902471, 1.012657825809694, 1.012657822677378, 1.0126578196256013, 1.0126578166522622, 1.0126578137553142, 1.0126578109327662, 1.0126578081826785, 1.0126578055031643, 1.0126578028923865, 1.0126578003485571, 1.0126577978699354, 1.0126577954548253, 1.012657793101578, 1.0126577908085874, 1.0126577885742891, 1.0126577863971615, 1.0126577842757216, 1.0126577822085268, 1.0126577801941719, 1.012657778231289, 1.012657776318547, 1.0126577744546479, 1.0126577726383292, 1.012657770868362, 1.012657769143548, 1.0126577674627228, 1.0126577658247506, 1.0126577642285257, 1.0126577626729723, 1.0126577611570415, 1.0126577596797133, 1.012657758239993, 1.012657756836913, 1.0126577554695304, 1.012657754136927, 1.012657752838209, 1.0126577515725053, 1.0126577503389678, 1.0126577491367705, 1.0126577479651089, 1.0126577468231996, 1.0126577457102797, 1.0126577446256058, 1.0126577435684534, 1.0126577425381178, 1.0126577415339129, 1.012657740555168, 1.0126577396012333, 1.0126577386714728, 1.0126577377652686, 1.0126577368820187, 1.0126577360211362, 1.0126577351820492, 1.012657734364202, 1.0126577335670515, 1.0126577327900697, 1.0126577320327415, 1.0126577312945664, 1.0126577305750555, 1.012657729873733, 1.0126577291901355, 1.0126577285238114, 1.0126577278743205, 1.0126577272412347, 1.012657726624136, 1.0126577260226175, 1.0126577254362834, 1.012657724864747, 1.012657724307632, 1.0126577237645726, 1.0126577232352107, 1.012657722719199, 1.0126577222161979, 1.0126577217258772, 1.0126577212479155, 1.0126577207819984, 1.012657720327821, 1.0126577198850852, 1.0126577194535007, 1.0126577190327852, 1.0126577186226624, 1.0126577182228649, 1.0126577178331306, 1.0126577174532048, 1.0126577170828388, 1.012657716721791, 1.0126577163698254, 1.0126577160267118, 1.0126577156922267, 1.012657715366152, 1.0126577150482747, 1.0126577147383875, 1.0126577144362883, 1.0126577141417807, 1.0126577138546728, 1.0126577135747774, 1.012657713301913, 1.0126577130359011, 1.0126577127765695, 1.0126577125237493, 1.012657712277276, 1.0126577120369893, 1.0126577118027338, 1.012657711574357, 1.0126577113517106, 1.0126577111346502, 1.0126577109230348, 1.0126577107167272, 1.012657710515594, 1.012657710319505, 1.0126577101283325, 1.0126577099419527, 1.0126577097602456, 1.0126577095830929, 1.0126577094103797, 1.0126577092419953, 1.0126577090778297, 1.0126577089177775, 1.0126577087617348, 1.012657708609601, 1.0126577084612776, 1.0126577083166688, 1.0126577081756813, 1.012657708038224, 1.012657707904208, 1.0126577077735472, 1.0126577076461571, 1.0126577075219556, 1.0126577074008625, 1.0126577072827998, 1.012657707167691, 1.0126577070554628, 1.0126577069460416, 1.0126577068393587, 1.0126577067353437, 1.0126577066339304, 1.0126577065350533, 1.0126577064386488, 1.0126577063446551, 1.0126577062530115, 1.0126577061636592, 1.0126577060765407, 1.0126577059915998, 1.012657705908782, 1.0126577058280342, 1.0126577057493045, 1.0126577056725423, 1.0126577055976975, 1.0126577055247235, 1.0126577054535726, 1.0126577053841994, 1.0126577053165589, 1.012657705250608, 1.0126577051863044, 1.012657705123607, 1.012657705062475, 1.0126577050028698, 1.0126577049447532, 1.0126577048880878, 1.0126577048328365, 1.012657704778965, 1.0126577047264385, 1.0126577046752232, 1.012657704625286, 1.0126577045765957, 1.0126577045291203, 1.0126577044828295, 1.0126577044376943, 1.0126577043936855, 1.0126577043507747, 1.0126577043089342, 1.0126577042681382, 1.0126577042283595, 1.0126577041895737, 1.0126577041517548, 1.0126577041148797, 1.0126577040789244, 1.0126577040438656, 1.0126577040096814, 1.0126577039763498, 1.0126577039438494, 1.0126577039121591, 1.0126577038812594, 1.0126577038511302, 1.012657703821752, 1.0126577037931064, 1.0126577037651752, 1.0126577037379398, 1.0126577037113837, 1.0126577036854896, 1.0126577036602409, 1.0126577036356215, 1.0126577036116156, 1.012657703588208, 1.0126577035653843, 1.012657703543129, 1.0126577035214284, 1.0126577035002684, 1.012657703479636, 1.0126577034595172, 1.0126577034399, 1.0126577034207718, 1.01265770340212, 1.0126577033839328, 1.0126577033661988, 1.0126577033489066, 1.012657703332045, 1.0126577033156035, 1.0126577032995716, 1.012657703283939, 1.012657703268696, 1.0126577032538324, 1.012657703239339, 1.0126577032252067, 1.0126577032114261, 1.012657703197989, 1.0126577031848862, 1.0126577031721098, 1.012657703159652, 1.0126577031475041, 1.0126577031356585, 1.0126577031241077, 1.0126577031128452, 1.0126577031018624, 1.0126577030911534, 1.0126577030807111, 1.0126577030705284, 1.0126577030605997, 1.0126577030509176, 1.0126577030414767, 1.012657703032271, 1.0126577030232942, 1.0126577030145412, 1.0126577030060058, 1.012657702997683, 1.0126577029895671, 1.0126577029816537, 1.0126577029739368, 1.012657702966412, 1.0126577029590749, 1.0126577029519201, 1.012657702944943, 1.01265770293814, 1.012657702931506, 1.0126577029250374, 1.0126577029187298, 1.0126577029125787, 1.0126577029065809, 1.0126577029007324, 1.0126577028950294, 1.0126577028894683, 1.0126577028840458, 1.0126577028787578, 1.0126577028736015, 1.0126577028685735, 1.0126577028636707, 1.0126577028588897, 1.0126577028542278, 1.012657702849682, 1.012657702845249, 1.0126577028409263, 1.0126577028367112, 1.012657702832601, 1.0126577028285928, 1.0126577028246846, 1.0126577028208739, 1.0126577028171573, 1.0126577028135335, 1.01265770281, 1.0126577028065542, 1.012657702803194, 1.0126577027999173, 1.0126577027967225, 1.0126577027936068, 1.0126577027905685, 1.0126577027876063, 1.0126577027847172, 1.0126577027819004, 1.0126577027791535, 1.0126577027764752, 1.012657702773863, 1.0126577027713157, 1.012657702768832, 1.0126577027664103, 1.0126577027640484, 1.0126577027617456, 1.0126577027594998, 1.01265770275731, 1.0126577027551744, 1.012657702753092, 1.0126577027510617, 1.0126577027490817, 1.012657702747151, 1.0126577027452681, 1.0126577027434318, 1.0126577027416417, 1.0126577027398957, 1.0126577027381933, 1.012657702736533, 1.0126577027349146, 1.0126577027333359, 1.0126577027317967, 1.0126577027302954, 1.012657702728832, 1.0126577027274044, 1.0126577027260126, 1.0126577027246553, 1.0126577027233319, 1.0126577027220414, 1.0126577027207828, 1.0126577027195558, 1.0126577027183588, 1.012657702717192, 1.012657702716054, 1.0126577027149444, 1.0126577027138621, 1.012657702712807, 1.012657702711778, 1.0126577027107748, 1.012657702709796, 1.0126577027088424, 1.0126577027079118, 1.0126577027070045, 1.0126577027061199, 1.0126577027052572, 1.012657702704416, 1.0126577027035955, 1.0126577027027956, 1.0126577027020156, 1.0126577027012547, 1.0126577027005128, 1.0126577026997894, 1.0126577026990842, 1.0126577026983963, 1.0126577026977257, 1.0126577026970716, 1.0126577026964338, 1.012657702695812, 1.0126577026952055, 1.012657702694614, 1.012657702694037, 1.0126577026934747, 1.0126577026929262, 1.0126577026923913, 1.01265770269187, 1.0126577026913615, 1.0126577026908654, 1.0126577026903822, 1.0126577026899106, 1.0126577026894505, 1.0126577026890025, 1.012657702688565, 1.0126577026881387, 1.0126577026877228, 1.0126577026873176, 1.012657702686922, 1.0126577026865364, 1.0126577026861605, 1.012657702685794, 1.0126577026854364, 1.0126577026850878, 1.0126577026847476, 1.0126577026844161, 1.0126577026840928, 1.0126577026837777, 1.0126577026834704, 1.0126577026831705, 1.0126577026828782, 1.0126577026825934, 1.012657702682315, 1.0126577026820442, 1.0126577026817796, 1.012657702681522, 1.0126577026812706, 1.0126577026810255, 1.0126577026807864, 1.0126577026805534, 1.012657702680326, 1.0126577026801045, 1.0126577026798882, 1.0126577026796775, 1.012657702679472, 1.0126577026792716, 1.0126577026790762, 1.0126577026788854, 1.0126577026786998, 1.0126577026785184, 1.0126577026783417, 1.0126577026781696, 1.0126577026780015, 1.0126577026778376, 1.0126577026776777, 1.0126577026775219, 1.01265770267737, 1.0126577026772217, 1.012657702677077, 1.0126577026769366, 1.0126577026767989, 1.012657702676665, 1.0126577026765342, 1.0126577026764068, 1.0126577026762826, 1.0126577026761614, 1.012657702676043, 1.012657702675928, 1.0126577026758157, 1.012657702675706, 1.0126577026755992, 1.012657702675495, 1.0126577026753938, 1.0126577026752943, 1.0126577026751977, 1.0126577026751036, 1.0126577026750117, 1.0126577026749222, 1.012657702674835, 1.0126577026747496, 1.0126577026746666, 1.0126577026745853, 1.0126577026745065, 1.0126577026744294, 1.0126577026743544, 1.0126577026742813, 1.0126577026742096, 1.01265770267414, 1.0126577026740722, 1.012657702674006, 1.0126577026739414, 1.0126577026738783, 1.0126577026738166, 1.0126577026737569, 1.0126577026736987, 1.0126577026736416, 1.012657702673586, 1.0126577026735317, 1.012657702673479, 1.0126577026734274, 1.0126577026733772, 1.012657702673328, 1.0126577026732808, 1.012657702673234, 1.0126577026731882, 1.012657702673144, 1.012657702673101, 1.0126577026730588, 1.012657702673018, 1.0126577026729782, 1.0126577026729389, 1.0126577026729007, 1.0126577026728634, 1.0126577026728274, 1.012657702672792, 1.0126577026727577, 1.012657702672724, 1.012657702672691, 1.0126577026726595, 1.0126577026726282, 1.0126577026725978, 1.0126577026725683, 1.0126577026725394, 1.012657702672511, 1.0126577026724837, 1.012657702672457, 1.0126577026724308, 1.0126577026724055, 1.0126577026723804, 1.0126577026723567, 1.0126577026723327, 1.0126577026723096, 1.0126577026722874, 1.0126577026722652, 1.0126577026722439, 1.012657702672223, 1.0126577026722028, 1.012657702672183, 1.012657702672164, 1.012657702672145, 1.0126577026721264, 1.0126577026721086, 1.012657702672091, 1.012657702672074, 1.0126577026720576, 1.0126577026720414, 1.0126577026720256, 1.0126577026720103, 1.0126577026719952, 1.0126577026719805, 1.0126577026719663, 1.0126577026719525, 1.0126577026719388, 1.0126577026719255, 1.0126577026719126, 1.0126577026719001, 1.0126577026718875, 1.012657702671876, 1.0126577026718642, 1.0126577026718528, 1.0126577026718417, 1.0126577026718306, 1.0126577026718202, 1.01265770267181, 1.0126577026717998, 1.01265770267179, 1.0126577026717807, 1.0126577026717714, 1.0126577026717622, 1.0126577026717534, 1.0126577026717447, 1.0126577026717363, 1.012657702671728, 1.01265770267172, 1.0126577026717123, 1.012657702671705, 1.0126577026716974, 1.0126577026716899, 1.012657702671683, 1.012657702671676, 1.0126577026716694, 1.0126577026716628, 1.0126577026716566, 1.0126577026716503, 1.0126577026716441, 1.0126577026716381, 1.0126577026716324, 1.0126577026716266, 1.0126577026716213, 1.012657702671616, 1.0126577026716108, 1.0126577026716055, 1.0126577026716006, 1.0126577026715957, 1.012657702671591, 1.0126577026715862, 1.012657702671582, 1.0126577026715773, 1.0126577026715733, 1.0126577026715693, 1.0126577026715653, 1.012657702671561, 1.0126577026715575, 1.0126577026715535, 1.0126577026715498, 1.0126577026715464, 1.0126577026715429, 1.0126577026715393, 1.0126577026715362, 1.0126577026715329, 1.0126577026715295, 1.0126577026715267, 1.0126577026715236, 1.0126577026715207, 1.0126577026715178, 1.0126577026715151, 1.0126577026715122, 1.0126577026715098, 1.0126577026715071, 1.012657702671505, 1.0126577026715022, 1.0126577026714996, 1.0126577026714976, 1.0126577026714951, 1.012657702671493, 1.0126577026714907, 1.0126577026714887, 1.0126577026714867, 1.012657702671485, 1.0126577026714827, 1.0126577026714807, 1.0126577026714787, 1.0126577026714771, 1.0126577026714751, 1.0126577026714736, 1.012657702671472, 1.0126577026714703, 1.0126577026714685, 1.0126577026714672, 1.0126577026714658, 1.012657702671464, 1.0126577026714627, 1.0126577026714614, 1.0126577026714596, 1.0126577026714583, 1.0126577026714574, 1.012657702671456, 1.0126577026714547, 1.0126577026714534, 1.0126577026714525, 1.0126577026714512, 1.01265770267145, 1.012657702671449, 1.0126577026714476, 1.0126577026714467, 1.0126577026714458, 1.012657702671445, 1.012657702671444, 1.0126577026714432, 1.0126577026714418, 1.0126577026714412, 1.01265770267144, 1.0126577026714394, 1.0126577026714383, 1.0126577026714376, 1.012657702671437, 1.0126577026714363, 1.0126577026714354, 1.0126577026714347, 1.0126577026714338, 1.0126577026714332, 1.0126577026714325, 1.012657702671432, 1.0126577026714312, 1.0126577026714307, 1.0126577026714298, 1.0126577026714294, 1.012657702671429, 1.012657702671428, 1.0126577026714276, 1.0126577026714272, 1.0126577026714267, 1.0126577026714263, 1.0126577026714254, 1.012657702671425, 1.0126577026714245, 1.0126577026714243, 1.0126577026714236, 1.0126577026714234, 1.012657702671423, 1.0126577026714225, 1.012657702671422, 1.0126577026714219, 1.0126577026714212, 1.012657702671421, 1.0126577026714205, 1.01265770267142, 1.0126577026714196, 1.0126577026714192, 1.0126577026714192, 1.0126577026714187, 1.0126577026714183, 1.0126577026714183, 1.0126577026714179, 1.0126577026714174, 1.012657702671417, 1.012657702671417, 1.0126577026714167, 1.0126577026714165, 1.012657702671416, 1.0126577026714159, 1.0126577026714156, 1.0126577026714152, 1.0126577026714152, 1.012657702671415, 1.0126577026714148, 1.0126577026714143, 1.0126577026714143, 1.0126577026714139, 1.0126577026714136, 1.0126577026714136, 1.0126577026714134, 1.0126577026714134, 1.012657702671413, 1.0126577026714128, 1.0126577026714128, 1.0126577026714125, 1.0126577026714125, 1.012657702671412, 1.012657702671412, 1.012657702671412, 1.0126577026714116, 1.0126577026714116, 1.0126577026714114, 1.0126577026714112, 1.0126577026714112, 1.0126577026714112, 1.012657702671411, 1.0126577026714108, 1.0126577026714105, 1.0126577026714105, 1.0126577026714103, 1.0126577026714103, 1.01265770267141, 1.01265770267141, 1.01265770267141, 1.0126577026714099, 1.0126577026714099, 1.0126577026714096, 1.0126577026714099]\n",
      "\n",
      "ReLU Activation Function:\n",
      "Final weights: [0.34482759 0.34482759]\n",
      "Number of epochs: 1000\n",
      "Error list: [0.561225, 0.505409380625, 0.4647647109088906, 0.43518351215076556, 0.41366739839317557, 0.3980280993241367, 0.38666899976157965, 0.37842552402711005, 0.3724485248326739, 0.36811909882352434, 0.3649863666574371, 0.36272203294917627, 0.3610872060831153, 0.3599081746458421, 0.359058726559927, 0.3584472469986478, 0.35800730622138577, 0.35769079566021517, 0.3574629243166645, 0.3572985729440351, 0.3571796389788474, 0.35709310418260803, 0.35702962928419946, 0.3569825327529264, 0.35694704942987987, 0.35691979293590864, 0.35689836636108024, 0.3568810807715142, 0.35686675204192025, 0.3568545545300192, 0.35684391595163767, 0.35683444207673776, 0.3568258629736617, 0.3568179947932129, 0.3568107127335266, 0.35680393202734273, 0.3567975946666947, 0.3567916602148609, 0.35678609951638895, 0.35678089045034495, 0.3567760151141097, 0.3567714580001737, 0.3567672048547912, 0.35676324199839604, 0.3567595559530733, 0.3567561332692034, 0.35675296047678673, 0.35675002411067147, 0.356747310775639, 0.35674480722903623, 0.35674250046679706, 0.35674037780429463, 0.3567384269472611, 0.35673663605053196, 0.3567349937639984, 0.35673348926616455, 0.3567321122862935, 0.35673085311643454, 0.3567297026147383, 0.35672865220146666, 0.3567276938490322, 0.35672682006728323, 0.35672602388512, 0.3567252988293871, 0.3567246389018499, 0.35672403855493656, 0.3567234926668113, 0.35672299651623907, 0.35672254575761464, 0.35672213639644546, 0.3567217647655179, 0.35672142750191116, 0.3567211215249847, 0.3567208440154188, 0.3567205923953619, 0.3567203643097048, 0.35672015760849224, 0.3567199703304534, 0.3567198006876311, 0.35671964705107323, 0.35671950793754537, 0.35671938199722, 0.3567192680022912, 0.35671916483646593, 0.35671907148527826, 0.3567189870271769, 0.3567189106253348, 0.3567188415201307, 0.3567187790222579, 0.35671872250641085, 0.3567186714055101, 0.3567186252054229, 0.3567185834401398, 0.3567185456873739, 0.35671851156454604, 0.3567184807251256, 0.35671845285529735, 0.35671842767092743, 0.3567184049148017, 0.35671838435411485, 0.35671836577818766, 0.3567183489963925, 0.35671833383626955, 0.35671832014181576, 0.35671830777193275, 0.3567182965990173, 0.3567182865076842, 0.3567182773936065, 0.3567182691624662, 0.3567182617290012, 0.3567182550161429, 0.35671824895423493, 0.35671824348032466, 0.35671823853752316, 0.3567182340744237, 0.3567182300445775, 0.3567182264060178, 0.35671822312082935, 0.35671822015475996, 0.3567182174768675, 0.3567182150592019, 0.3567182128765169, 0.35671821090600936, 0.3567182091270841, 0.3567182075211402, 0.35671820607137955, 0.3567182047626315, 0.35671820358119666, 0.3567182025147039, 0.3567182015519823, 0.35671820068294496, 0.35671819989848386, 0.3567181991903753, 0.3567181985511942, 0.3567181979742364, 0.3567181974534492, 0.35671819698336793, 0.3567181965590591, 0.3567181961760688, 0.35671819583037573, 0.3567181955183499, 0.3567181952367143, 0.3567181949825105, 0.35671819475306793, 0.3567181945459758, 0.3567181943590579, 0.35671819419034967, 0.3567181940380779, 0.35671819390064186, 0.3567181937765966, 0.3567181936646378, 0.35671819356358825, 0.35671819347238526, 0.3567181933900696, 0.3567181933157755, 0.35671819324872156, 0.3567181931882022, 0.3567181931335809, 0.3567181930842829, 0.35671819303978947, 0.35671819299963253, 0.3567181929633895, 0.3567181929306791, 0.3567181929011568, 0.356718192874512, 0.3567181928504644, 0.3567181928287608, 0.35671819280917255, 0.3567181927914939, 0.35671819277553846, 0.3567181927611383, 0.3567181927481419, 0.35671819273641237, 0.3567181927258264, 0.3567181927162722, 0.3567181927076495, 0.3567181926998672, 0.35671819269284366, 0.35671819268650484, 0.356718192680784, 0.3567181926756208, 0.35671819267096105, 0.3567181926667554, 0.3567181926629599, 0.3567181926595343, 0.3567181926564428, 0.3567181926536526, 0.35671819265113447, 0.35671819264886184, 0.35671819264681076, 0.35671819264495963, 0.35671819264328897, 0.3567181926417812, 0.3567181926404204, 0.35671819263919236, 0.3567181926380839, 0.3567181926370837, 0.3567181926361808, 0.3567181926353661, 0.3567181926346308, 0.35671819263396715, 0.35671819263336824, 0.35671819263282767, 0.35671819263233984, 0.3567181926318995, 0.3567181926315022, 0.3567181926311436, 0.3567181926308199, 0.3567181926305278, 0.3567181926302642, 0.35671819263002624, 0.3567181926298115, 0.3567181926296178, 0.3567181926294429, 0.35671819262928506, 0.3567181926291426, 0.35671819262901405, 0.356718192628898, 0.3567181926287933, 0.35671819262869875, 0.3567181926286135, 0.35671819262853655, 0.35671819262846705, 0.3567181926284043, 0.35671819262834775, 0.35671819262829674, 0.3567181926282506, 0.35671819262820903, 0.3567181926281715, 0.3567181926281376, 0.35671819262810706, 0.3567181926280794, 0.3567181926280545, 0.356718192628032, 0.35671819262801174, 0.35671819262799354, 0.356718192627977, 0.356718192627962, 0.35671819262794857, 0.35671819262793636, 0.3567181926279255, 0.35671819262791554, 0.3567181926279067, 0.35671819262789856, 0.3567181926278913, 0.35671819262788473, 0.35671819262787885, 0.3567181926278735, 0.35671819262786864, 0.3567181926278643, 0.35671819262786036, 0.35671819262785687, 0.35671819262785365, 0.35671819262785076, 0.3567181926278482, 0.35671819262784576, 0.3567181926278437, 0.35671819262784177, 0.3567181926278401, 0.3567181926278384, 0.3567181926278371, 0.3567181926278358, 0.3567181926278346, 0.35671819262783366, 0.35671819262783266, 0.3567181926278319, 0.3567181926278311, 0.3567181926278304, 0.3567181926278297, 0.3567181926278292, 0.35671819262782867, 0.35671819262782817, 0.3567181926278279, 0.35671819262782745, 0.35671819262782706, 0.3567181926278267, 0.35671819262782645, 0.3567181926278262, 0.356718192627826, 0.3567181926278259, 0.35671819262782567, 0.35671819262782556, 0.35671819262782534, 0.35671819262782517, 0.35671819262782506, 0.35671819262782495, 0.35671819262782484, 0.3567181926278249, 0.3567181926278248, 0.3567181926278246, 0.35671819262782467, 0.35671819262782456, 0.35671819262782445, 0.3567181926278244, 0.35671819262782434, 0.3567181926278244, 0.35671819262782434, 0.3567181926278243, 0.35671819262782434, 0.3567181926278242, 0.3567181926278242, 0.3567181926278243, 0.3567181926278242, 0.3567181926278242, 0.35671819262782417, 0.3567181926278241, 0.3567181926278241, 0.3567181926278241, 0.3567181926278241, 0.35671819262782406, 0.3567181926278241, 0.35671819262782417, 0.3567181926278241, 0.35671819262782406, 0.356718192627824, 0.356718192627824, 0.3567181926278241, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.3567181926278241, 0.35671819262782395, 0.35671819262782395, 0.35671819262782406, 0.3567181926278241, 0.35671819262782395, 0.35671819262782395, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.35671819262782395, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.35671819262782395, 0.356718192627824, 0.356718192627824, 0.35671819262782395, 0.35671819262782395, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782395, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782395, 0.356718192627824, 0.356718192627824, 0.35671819262782395, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782395, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.35671819262782395, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.356718192627824, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.35671819262782395, 0.356718192627824, 0.356718192627824, 0.35671819262782395, 0.35671819262782395, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.356718192627824, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.35671819262782395, 0.35671819262782395, 0.356718192627824, 0.356718192627824, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.356718192627824, 0.35671819262782406, 0.35671819262782395, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.35671819262782395, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.35671819262782395, 0.35671819262782395, 0.356718192627824, 0.356718192627824, 0.35671819262782395, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782395, 0.35671819262782395, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782395, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782395, 0.356718192627824, 0.356718192627824, 0.35671819262782395, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782395, 0.356718192627824, 0.35671819262782406, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.356718192627824, 0.356718192627824, 0.35671819262782395, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.35671819262782395, 0.35671819262782395, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782395, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.35671819262782395, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.356718192627824, 0.35671819262782395, 0.35671819262782395, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.356718192627824, 0.35671819262782395, 0.35671819262782395, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.356718192627824, 0.356718192627824, 0.35671819262782406, 0.356718192627824, 0.356718192627824, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395, 0.35671819262782395]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def perceptron_training(inputs, targets, weights, learning_rate, activation_function):\n",
    "    epochs = 0\n",
    "    errors = []\n",
    "\n",
    "    while True:\n",
    "        total_error = 0\n",
    "        for i in range(len(inputs)):\n",
    "            weighted_sum = summation_unit(inputs[i], weights)\n",
    "            prediction = activation_function(weighted_sum)\n",
    "            error = targets[i] - prediction\n",
    "            total_error += error ** 2\n",
    "\n",
    "            # Update weights\n",
    "            for j in range(len(weights)):\n",
    "                weights[j] += learning_rate * error * inputs[i][j]\n",
    "\n",
    "        errors.append(total_error)\n",
    "        epochs += 1\n",
    "\n",
    "        if total_error <= 0.002 or epochs >= 1000:\n",
    "            break\n",
    "\n",
    "    return weights, epochs, errors\n",
    "\n",
    "# Example use case for AND gate logic\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "targets = np.array([0, 0, 0, 1])  # AND gate outputs\n",
    "initial_weights = np.array([0.1, 0.2])  # Initial weights\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Training with different activation functions\n",
    "activation_functions = {\n",
    "    'Bipolar Step': bipolar_step_function,\n",
    "    'Sigmoid': sigmoid_function,\n",
    "    'ReLU': relu_function\n",
    "}\n",
    "\n",
    "for name, activation_func in activation_functions.items():\n",
    "    final_weights, num_epochs, error_list = perceptron_training(inputs, targets, initial_weights.copy(), learning_rate, activation_func)\n",
    "    print(f\"{name} Activation Function:\")\n",
    "    print(\"Final weights:\", final_weights)\n",
    "    print(\"Number of epochs:\", num_epochs)\n",
    "    print(\"Error list:\", error_list)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45df967c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rates: [0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6000000000000001, 0.7000000000000001, 0.8, 0.9, 1.0]\n",
      "Iterations to converge: [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.1 * i for i in range(1, 11)]\n",
    "iterations_to_converge = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    _, epochs, _ = perceptron_training(inputs, targets, initial_weights, lr, step_function)\n",
    "    iterations_to_converge.append(epochs)\n",
    "\n",
    "print(\"Learning rates:\", learning_rates)\n",
    "print(\"Iterations to converge:\", iterations_to_converge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2faddf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR Gate - Final weights: [-5.00000000e-02 -2.77555756e-17]\n",
      "XOR Gate - Number of epochs: 1000\n",
      "XOR Gate - Error list: [2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "# XOR gate logic inputs and targets\n",
    "inputs_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "targets_xor = np.array([0, 1, 1, 0])  # XOR gate outputs\n",
    "\n",
    "final_weights_xor, num_epochs_xor, error_list_xor = perceptron_training(inputs_xor, targets_xor, initial_weights, learning_rate, step_function)\n",
    "print(\"XOR Gate - Final weights:\", final_weights_xor)\n",
    "print(\"XOR Gate - Number of epochs:\", num_epochs_xor)\n",
    "print(\"XOR Gate - Error list:\", error_list_xor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba87ea87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Data - Final weights: [-1.64971851 11.29275561 -0.28473673]\n",
      "Customer Data - Number of epochs: 19\n",
      "Customer Data - Error list: [4.9206059169848135, 5.682250374716909, 5.344411525293722, 5.986205519774653, 5.216716803351868, 4.085485488528227, 3.646993568213048, 3.9471845032844044, 3.9558890517994723, 3.956135622223944, 3.9549392546057818, 3.9534115700000427, 3.9514776791947055, 3.947558173217952, 3.9331855547263963, 3.866687213324688, 3.4858006792644227, 0.011557144961442624, 0.00014333326075675193]\n"
     ]
    }
   ],
   "source": [
    "# Assuming sigmoid as activation function and some initialized weights\n",
    "customer_data = np.array([\n",
    "    [20, 6, 2],\n",
    "    [16, 3, 6],\n",
    "    [27, 6, 2],\n",
    "    [19, 1, 2],\n",
    "    [24, 4, 2],\n",
    "    [22, 1, 5],\n",
    "    [15, 4, 2],\n",
    "    [18, 4, 2],\n",
    "    [21, 1, 4],\n",
    "    [16, 2, 4]\n",
    "])\n",
    "targets_customers = np.array([1, 1, 1, 0, 1, 0, 1, 1, 0, 0])  # High value (Yes/No as 1/0)\n",
    "\n",
    "initial_weights_customers = np.random.rand(3)\n",
    "learning_rate_customers = 0.1\n",
    "\n",
    "final_weights_customers, num_epochs_customers, error_list_customers = perceptron_training(customer_data, targets_customers, initial_weights_customers, learning_rate_customers, sigmoid_function)\n",
    "print(\"Customer Data - Final weights:\", final_weights_customers)\n",
    "print(\"Customer Data - Number of epochs:\", num_epochs_customers)\n",
    "print(\"Customer Data - Error list:\", error_list_customers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5100d02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights using Pseudo-Inverse: [-0.25  0.5   0.5 ]\n",
      "Weights using Perceptron Training: [-8.56605868  5.60159393  5.59549022]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example use case for AND gate logic\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "targets = np.array([0, 0, 0, 1])  # AND gate outputs\n",
    "\n",
    "# Adding bias to inputs\n",
    "inputs_with_bias = np.c_[np.ones((inputs.shape[0], 1)), inputs]\n",
    "\n",
    "# Pseudo-inverse method to calculate weights\n",
    "pseudo_inverse_weights = np.linalg.pinv(inputs_with_bias) @ targets\n",
    "print(\"Weights using Pseudo-Inverse:\", pseudo_inverse_weights)\n",
    "\n",
    "# Training Perceptron using Sigmoid activation function\n",
    "initial_weights = np.random.rand(3)\n",
    "learning_rate = 0.1\n",
    "\n",
    "def sigmoid_function(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "final_weights_perceptron, num_epochs_perceptron, error_list_perceptron = perceptron_training(inputs_with_bias, targets, initial_weights, learning_rate, sigmoid_function)\n",
    "\n",
    "print(\"Weights using Perceptron Training:\", final_weights_perceptron)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "978d32e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights: [-2.95332975  1.87505105  1.86313364]\n",
      "Error list: [0.37327401003366845, 0.36845151161012285, 0.36363816814717337, 0.3588390004688757, 0.35405906351450617, 0.34930341857613073, 0.3445771046726127, 0.33988510941863714, 0.335232339770839, 0.33062359304815064, 0.3260635286291604, 0.32155664072505336, 0.317107232612429, 0.3127193926861685, 0.30839697265918814, 0.30414356819433874, 0.2999625022052261, 0.29585681100892036, 0.29182923345616246, 0.2878822031056504, 0.28401784345020553, 0.28023796614586083, 0.27654407214188353, 0.27293735556183774, 0.26941871014419044, 0.26598873801648576, 0.26264776055029865, 0.25939583102519453, 0.2562327488186875, 0.25315807483530484, 0.25017114789075856, 0.24727110177610348, 0.24445688274075617, 0.24172726715139287, 0.23908087910506667, 0.2365162077984274, 0.2340316244798037, 0.23162539883628547, 0.22929571469313298, 0.22704068492721866, 0.22485836551930785, 0.2227467686914234, 0.22070387509506753, 0.21872764503351264, 0.2168160287166584, 0.2149669755600724, 0.21317844255083876, 0.21144840171185025, 0.20977484670330943, 0.20815579860563704, 0.20658931093187693, 0.20507347392022998, 0.2036064181587162, 0.20218631759433708, 0.20081139197864656, 0.19947990880049749, 0.19819018475504857, 0.19694058679601825, 0.19572953281576835, 0.1945554919951834, 0.1934169848625672, 0.1923125830979702, 0.19124090911654645, 0.19020063546176555, 0.1891904840366041, 0.18820922519824512, 0.1872556767393384, 0.18632870277653552, 0.18542721256482328, 0.1845501592541332, 0.18369653860281646, 0.18286538766083035, 0.18205578343388804, 0.18126684153836808, 0.1804977148554618, 0.17974759219184222, 0.17901569695306524, 0.17830128583495208, 0.17760364753734204, 0.1769221015038407, 0.17625599669051273, 0.17560471036586983, 0.17496764694398065, 0.1743442368520695, 0.17373393543357235, 0.17313622188726932, 0.17255059824281732, 0.17197658837274737, 0.1714137370407754, 0.17086160898608918, 0.1703197880431219, 0.16978787629619202, 0.169265493268287, 0.16875227514318303, 0.16824787402002564, 0.16775195719944652, 0.1672642065002528, 0.1667843176057, 0.16631199943834288, 0.16584697356245248, 0.16538897361298438, 0.16493774475009304, 0.1644930431381944, 0.16405463544859553, 0.16362229838472975, 0.16319581822905593, 0.16277499041070387, 0.16235961909297483, 0.16194951677983271, 0.16154450394054679, 0.16114440865167942, 0.16074906625563673, 0.16035831903503198, 0.15997201590213786, 0.15959001210273469, 0.15921216893368711, 0.15883835347361172, 0.1584684383260226, 0.15810230137437092, 0.15773982554841798, 0.15738089860140647, 0.15702541289751953, 0.15667326520914002, 0.15632435652344384, 0.15597859185788476, 0.15563588008414647, 0.1552961337601595, 0.15495926896979906, 0.15462520516989692, 0.1542938650442207, 0.15396517436408783, 0.15363906185529913, 0.1533154590710916, 0.15299430027082506, 0.1526755223041304, 0.1523590645002615, 0.15204486856240446, 0.1517328784667104, 0.15142304036582957, 0.151115302496735, 0.15080961509263502, 0.15050593029878245, 0.1502042020919998, 0.1499043862037458, 0.1496064400465605, 0.1493103226437312, 0.14901599456203035, 0.14872341784738496, 0.14843255596334112, 0.1481433737321969, 0.1478558372786809, 0.14756991397606073, 0.1472855723945709, 0.14700278225205488, 0.1467215143667217, 0.1464417406119215, 0.1461634338728491, 0.1458865680050903, 0.14561111779492758, 0.1453370589213271, 0.14506436791953306, 0.14479302214619783, 0.14452299974597957, 0.1442542796195443, 0.1439868413929092, 0.14372066538806974, 0.143455732594854, 0.1431920246439511, 0.14292952378106316, 0.14266821284213138, 0.1424080752295912, 0.14214909488961086, 0.141891256290273, 0.1416345444006571, 0.1413789446707859, 0.1411244430123984, 0.14087102578051422, 0.14061867975575626, 0.14036739212739993, 0.1401171504771171, 0.13986794276338732, 0.13961975730654724, 0.13937258277445141, 0.13912640816872035, 0.13888122281154924, 0.13863701633305658, 0.13839377865914776, 0.13815149999987464, 0.13791017083826843, 0.13766978191962878, 0.13743032424124801, 0.13719178904255375, 0.13695416779565342, 0.13671745219626213, 0.13648163415500097, 0.1362467057890478, 0.13601265941412824, 0.1357794875368325, 0.13554718284724365, 0.1353157382118666, 0.1350851466668434, 0.13485540141144567, 0.13462649580183073, 0.1343984233450518, 0.1341711776933124, 0.13394475263845357, 0.13371914210666647, 0.1334943401534194, 0.13327034095859197, 0.13304713882180716, 0.13282472815795376, 0.13260310349289134, 0.1323822594593303, 0.13216219079288022, 0.1319428923282595, 0.1317243589956597, 0.13150658581725838, 0.131289567903875, 0.131073300451763, 0.13085777873953358, 0.13064299812520552, 0.1304289540433755, 0.130215642002505, 0.13000305758231792, 0.1297911964313055, 0.12958005426433328, 0.12936962686034717, 0.1291599100601727, 0.12895089976440588, 0.12874259193139015, 0.12853498257527723, 0.12832806776416741, 0.12812184361832663, 0.12791630630847722, 0.12771145205415918, 0.12750727712215884, 0.1273037778250028, 0.12710095051951348, 0.1268987916054254, 0.12669729752405734, 0.12649646475704046, 0.1262962898250985, 0.12609676928687838, 0.12589789973782944, 0.12569967780912908, 0.12550210016665214, 0.1253051635099839, 0.12510886457147302, 0.12491320011532404, 0.12471816693672748, 0.12452376186102533, 0.12432998174291157, 0.12413682346566529, 0.12394428394041544, 0.12375236010543614, 0.12356104892547046, 0.12337034739108252, 0.12318025251803583, 0.1229907613466972, 0.12280187094146501, 0.12261357839022088, 0.12242588080380375, 0.12223877531550506, 0.1220522590805849, 0.12186632927580723, 0.12168098309899444, 0.12149621776859935, 0.12131203052329467, 0.12112841862157872, 0.12094537934139729, 0.12076290997977998, 0.12058100785249135, 0.12039967029369567, 0.12021889465563484, 0.12003867830831891, 0.11985901863922883, 0.11967991305303016, 0.11950135897129863, 0.11932335383225531, 0.11914589509051254, 0.1189689802168291, 0.11879260669787457, 0.1186167720360028, 0.11844147374903324, 0.11826670937004062, 0.11809247644715237, 0.11791877254335295, 0.11774559523629557, 0.1175729421181203, 0.11740081079527867, 0.11722919888836428, 0.11705810403194938, 0.11688752387442669, 0.11671745607785675, 0.1165478983178203, 0.11637884828327524, 0.11621030367641856, 0.1160422622125524, 0.11587472161995416, 0.1157076796397511, 0.11554113402579827, 0.1153750825445602, 0.11520952297499629, 0.11504445310844905, 0.11487987074853605, 0.11471577371104433, 0.11455215982382819, 0.11438902692670916, 0.11422637287137921, 0.11406419552130587, 0.1139024927516401, 0.11374126244912623, 0.11358050251201411, 0.1134202108499734, 0.1132603853840097, 0.11310102404638255, 0.11294212478052532, 0.11278368554096671, 0.11262570429325391, 0.11246817901387732, 0.11231110769019663, 0.11215448832036849, 0.11199831891327539, 0.11184259748845585, 0.1116873220760358, 0.11153249071666135, 0.11137810146143218, 0.11122415237183648, 0.11107064151968687, 0.1109175669870567, 0.11076492686621814, 0.1106127192595805, 0.11046094227962971, 0.11030959404886845, 0.1101586726997571, 0.11000817637465549, 0.10985810322576536, 0.10970845141507327, 0.10955921911429446, 0.10941040450481723, 0.10926200577764772, 0.1091140211333555, 0.10896644878201972, 0.1088192869431755, 0.10867253384576116, 0.1085261877280657, 0.10838024683767691, 0.10823470943142971, 0.10808957377535514, 0.10794483814462974, 0.10780050082352509, 0.10765656010535815, 0.10751301429244149, 0.10736986169603424, 0.10722710063629337, 0.10708472944222502, 0.10694274645163647, 0.10680115001108828, 0.1066599384758467, 0.10651911020983656, 0.10637866358559411, 0.10623859698422067, 0.10609890879533589, 0.10595959741703195, 0.10582066125582748, 0.1056820987266221, 0.10554390825265085, 0.1054060882654394, 0.10526863720475893, 0.10513155351858156, 0.10499483566303597, 0.10485848210236316, 0.10472249130887261, 0.10458686176289836, 0.10445159195275566, 0.10431668037469752, 0.10418212553287164, 0.10404792593927761, 0.10391408011372405, 0.10378058658378622, 0.10364744388476377, 0.10351465055963838, 0.10338220515903228, 0.10325010624116605, 0.10311835237181757, 0.10298694212428038, 0.1028558740793225, 0.1027251468251458, 0.10259475895734496, 0.1024647090788669, 0.10233499579997049, 0.10220561773818629, 0.10207657351827643, 0.10194786177219492, 0.10181948113904783, 0.1016914302650539, 0.10156370780350513, 0.10143631241472764, 0.101309242766043, 0.10118249753172891, 0.10105607539298106, 0.10092997503787458, 0.1008041951613256, 0.10067873446505349, 0.10055359165754268, 0.10042876545400517, 0.10030425457634268, 0.10018005775310967, 0.10005617371947577, 0.09993260121718897, 0.09980933899453873, 0.09968638580631926, 0.09956374041379304, 0.09944140158465446, 0.09931936809299372, 0.09919763871926077, 0.0990762122502296, 0.0989550874789624, 0.09883426320477438, 0.09871373823319823, 0.09859351137594909, 0.09847358145088957, 0.09835394728199504, 0.09823460769931894, 0.09811556153895837, 0.09799680764301977, 0.09787834485958494, 0.09776017204267702, 0.09764228805222674, 0.09752469175403886, 0.09740738201975868, 0.097290357726839, 0.09717361775850672, 0.09705716100373023, 0.09694098635718648, 0.09682509271922854, 0.09670947899585303, 0.09659414409866801, 0.096479086944861, 0.09636430645716679, 0.09624980156383592, 0.09613557119860322, 0.096021614300656, 0.09590792981460335, 0.09579451669044459, 0.09568137388353867, 0.09556850035457329, 0.09545589506953439, 0.09534355699967567, 0.09523148512148849, 0.09511967841667157, 0.09500813587210122, 0.09489685647980162, 0.09478583923691505, 0.0946750831456726, 0.09456458721336489, 0.09445435045231282, 0.0943443718798388, 0.0942346505182379, 0.09412518539474907, 0.09401597554152691, 0.09390701999561323, 0.09379831779890874, 0.09368986799814544, 0.09358166964485834, 0.09347372179535804, 0.09336602351070322, 0.09325857385667301, 0.09315137190374008, 0.0930444167270433, 0.09293770740636104, 0.09283124302608427, 0.09272502267519, 0.09261904544721493, 0.09251331044022883, 0.09240781675680873, 0.09230256350401275, 0.09219754979335437, 0.09209277474077648, 0.09198823746662597, 0.09188393709562845, 0.09177987275686277, 0.09167604358373596, 0.09157244871395841, 0.09146908728951879, 0.0913659584566596, 0.0912630613658525, 0.09116039517177393, 0.09105795903328104, 0.09095575211338726, 0.09085377357923856, 0.09075202260208956, 0.09065049835727988, 0.09054920002421063, 0.09044812678632085, 0.09034727783106442, 0.0902466523498867, 0.09014624953820197, 0.09004606859536993, 0.0899461087246737, 0.08984636913329666, 0.08974684903230021, 0.08964754763660163, 0.08954846416495155, 0.08944959783991219, 0.08935094788783526, 0.08925251353884031, 0.08915429402679292, 0.08905628858928334, 0.08895849646760497, 0.08886091690673312, 0.0887635491553039, 0.08866639246559319, 0.08856944609349571, 0.0884727092985043, 0.08837618134368933, 0.08827986149567812, 0.08818374902463458, 0.0880878432042389, 0.08799214331166742, 0.08789664862757275, 0.08780135843606363, 0.08770627202468534, 0.08761138868440008, 0.08751670770956713, 0.0874222283979238, 0.0873279500505659, 0.08723387197192861, 0.08713999346976753, 0.08704631385513947, 0.08695283244238393, 0.08685954854910405, 0.08676646149614833, 0.08667357060759187, 0.0865808752107182, 0.08648837463600084, 0.08639606821708523, 0.08630395529077078, 0.08621203519699266, 0.08612030727880432, 0.08602877088235941, 0.08593742535689461, 0.08584627005471163, 0.08575530433116035, 0.085664527544621, 0.08557393905648752, 0.08548353823114996, 0.08539332443597789, 0.08530329704130342, 0.08521345542040437, 0.08512379894948777, 0.08503432700767309, 0.08494503897697595, 0.08485593424229185, 0.08476701219137957, 0.08467827221484539, 0.08458971370612686, 0.08450133606147685, 0.08441313867994779, 0.08432512096337574, 0.08423728231636489, 0.08414962214627197, 0.08406213986319068, 0.08397483487993643, 0.083887706612031, 0.08380075447768741, 0.08371397789779483, 0.08362737629590336, 0.08354094909820955, 0.08345469573354122, 0.08336861563334289, 0.0832827082316611, 0.08319697296512998, 0.08311140927295663, 0.08302601659690687, 0.08294079438129093, 0.08285574207294932, 0.08277085912123862, 0.0826861449780176, 0.08260159909763312, 0.0825172209369065, 0.08243300995511965, 0.08234896561400143, 0.08226508737771397, 0.08218137471283937, 0.08209782708836602, 0.08201444397567552, 0.0819312248485293, 0.08184816918305542, 0.08176527645773558, 0.08168254615339202, 0.08159997775317464, 0.08151757074254803, 0.08143532460927906, 0.08135323884342366, 0.0812713129373146, 0.08118954638554887, 0.0811079386849751, 0.08102648933468132, 0.08094519783598256, 0.08086406369240876, 0.08078308640969227, 0.08070226549575629, 0.08062160046070244, 0.08054109081679894, 0.0804607360784689, 0.08038053576227826, 0.08030048938692444, 0.08022059647322427, 0.0801408565441028, 0.08006126912458161, 0.07998183374176734, 0.07990254992484053, 0.07982341720504409, 0.07974443511567222, 0.07966560319205933, 0.07958692097156878, 0.07950838799358195, 0.07943000379948734, 0.0793517679326696, 0.07927367993849876, 0.07919573936431959, 0.07911794575944064, 0.07904029867512395, 0.07896279766457426, 0.07888544228292865, 0.07880823208724605, 0.07873116663649697, 0.07865424549155309, 0.07857746821517707, 0.07850083437201245, 0.07842434352857348, 0.07834799525323508, 0.07827178911622279, 0.07819572468960298, 0.07811980154727292, 0.078044019264951, 0.0779683774201669, 0.07789287559225197, 0.07781751336232971, 0.07774229031330596, 0.07766720602985959, 0.07759226009843306, 0.07751745210722288, 0.0774427816461703, 0.07736824830695224, 0.07729385168297181, 0.0772195913693492, 0.07714546696291275, 0.07707147806218963, 0.07699762426739698, 0.07692390518043307, 0.07685032040486817, 0.0767768695459359, 0.07670355221052425, 0.07663036800716713, 0.07655731654603548, 0.07648439743892865, 0.07641161029926598, 0.07633895474207804, 0.07626643038399841, 0.07619403684325504, 0.07612177373966211, 0.07604964069461148, 0.07597763733106455, 0.07590576327354405, 0.07583401814812579, 0.07576240158243062, 0.07569091320561641, 0.07561955264836975, 0.07554831954289834, 0.07547721352292291, 0.07540623422366914, 0.07533538128186018, 0.07526465433570874, 0.0751940530249092, 0.07512357699063005, 0.07505322587550628, 0.07498299932363167, 0.07491289698055138, 0.07484291849325425, 0.07477306351016559, 0.07470333168113943, 0.0746337226574515, 0.07456423609179164, 0.07449487163825658, 0.07442562895234281, 0.07435650769093924, 0.0742875075123201, 0.07421862807613774, 0.0741498690434158, 0.07408123007654185, 0.07401271083926068, 0.0739443109966672, 0.07387603021519953, 0.07380786816263227, 0.07373982450806954, 0.07367189892193818, 0.0736040910759812, 0.0735364006432509, 0.07346882729810214, 0.07340137071618594, 0.07333403057444288, 0.0732668065510962, 0.07319969832564585, 0.0731327055788614, 0.07306582799277625, 0.0729990652506807, 0.07293241703711592, 0.07286588303786745, 0.07279946293995912, 0.07273315643164657, 0.07266696320241119, 0.072600882942954, 0.07253491534518927, 0.07246906010223872, 0.0724033169084253, 0.07233768545926716, 0.07227216545147172, 0.07220675658292969, 0.07214145855270904, 0.07207627106104936, 0.0720111938093558, 0.07194622650019328, 0.07188136883728084, 0.07181662052548567, 0.07175198127081761, 0.07168745078042334, 0.07162302876258075, 0.07155871492669337, 0.07149450898328472, 0.07143041064399289, 0.07136641962156476, 0.07130253562985076, 0.07123875838379935, 0.07117508759945163, 0.0711115229939357, 0.07104806428546169, 0.0709847111933161, 0.0709214634378568, 0.07085832074050741, 0.07079528282375233, 0.07073234941113148, 0.07066952022723508, 0.07060679499769844, 0.07054417344919711, 0.07048165530944137, 0.07041924030717156, 0.07035692817215275, 0.07029471863517009, 0.07023261142802346, 0.07017060628352277, 0.07010870293548292, 0.07004690111871903, 0.06998520056904142, 0.06992360102325103, 0.06986210221913426, 0.0698007038954586, 0.06973940579196751, 0.06967820764937598, 0.06961710920936569, 0.06955611021458036, 0.0694952104086211, 0.06943440953604191, 0.06937370734234488, 0.06931310357397587, 0.06925259797831979, 0.06919219030369615, 0.0691318802993546, 0.06907166771547049, 0.06901155230314038, 0.06895153381437755, 0.06889161200210786, 0.0688317866201652, 0.06877205742328714, 0.06871242416711076, 0.06865288660816835, 0.06859344450388291, 0.06853409761256422, 0.06847484569340456, 0.06841568850647436, 0.06835662581271809, 0.06829765737395034, 0.06823878295285135, 0.06818000231296327, 0.06812131521868572, 0.06806272143527212, 0.06800422072882532, 0.06794581286629385, 0.06788749761546782, 0.06782927474497498, 0.0677711440242767, 0.0677131052236643, 0.06765515811425493, 0.06759730246798767, 0.06753953805761996, 0.06748186465672337, 0.0674242820396802, 0.06736678998167946, 0.06730938825871308, 0.06725207664757232, 0.06719485492584396, 0.0671377228719065, 0.06708068026492675, 0.06702372688485589, 0.066966862512426, 0.06691008692914628, 0.06685339991729967, 0.06679680125993914, 0.06674029074088408, 0.06668386814471687, 0.06662753325677925, 0.06657128586316895, 0.06651512575073609, 0.06645905270707982, 0.06640306652054473, 0.06634716698021755, 0.06629135387592378, 0.0662356269982241, 0.06617998613841126, 0.0661244310885065, 0.06606896164125636, 0.06601357759012932, 0.0659582787293125, 0.0659030648537084, 0.06584793575893164, 0.06579289124130572, 0.06573793109785975, 0.0656830551263253, 0.0656282631251332, 0.06557355489341031, 0.06551893023097653, 0.06546438893834139, 0.06540993081670121, 0.06535555566793583, 0.06530126329460559, 0.06524705349994821, 0.0651929260878758, 0.06513888086297172, 0.06508491763048771, 0.06503103619634074, 0.06497723636711025, 0.06492351795003479, 0.06486988075300947, 0.06481632458458267, 0.06476284925395354, 0.06470945457096863, 0.06465614034611933, 0.06460290639053884, 0.0645497525159993, 0.06449667853490912, 0.06444368426030986, 0.0643907695058737, 0.06433793408590047, 0.06428517781531491, 0.06423250050966396, 0.06417990198511386, 0.06412738205844756, 0.06407494054706189, 0.06402257726896493, 0.06397029204277327, 0.06391808468770933, 0.06386595502359871, 0.06381390287086743, 0.0637619280505395, 0.06371003038423416, 0.06365820969416325, 0.06360646580312862, 0.0635547985345197, 0.06350320771231062, 0.06345169316105798, 0.06340025470589811, 0.06334889217254465, 0.0632976053872859, 0.06324639417698243, 0.06319525836906462, 0.06314419779152997, 0.063093212272941, 0.06304230164242236, 0.06299146572965877, 0.06294070436489241, 0.06289001737892054, 0.0628394046030931, 0.06278886586931036, 0.0627384010100204, 0.062688009858217, 0.06263769224743707, 0.06258744801175847, 0.06253727698579752, 0.06248717900470681, 0.06243715390417287, 0.062387201520413924, 0.06233732169017752, 0.06228751425073829, 0.06223777903989578, 0.062188115895972076, 0.062138524657809724, 0.062089005164769395, 0.06203955725672772, 0.06199018077407508, 0.061940875557713465, 0.061891641449054305, 0.06184247829001617, 0.061793385923022796, 0.06174436419100089, 0.06169541293737793, 0.061646532006080175, 0.06159772124153044, 0.061548980488646064, 0.06150030959283684, 0.06145170840000286, 0.06140317675653255, 0.06135471450930061, 0.0613063215056658, 0.06125799759346921, 0.061209742621031946, 0.061161556437153354, 0.061113438891108776, 0.06106538983264782, 0.061017409111992285, 0.060969496579834, 0.060921652087333175, 0.06087387548611621, 0.06082616662827395, 0.06077852536635954, 0.06073095155338672, 0.06068344504282773, 0.060636005688611556, 0.060588633345121976, 0.060541327867195606, 0.06049408911012016, 0.06044691692963253, 0.06039981118191694, 0.060352771723603045, 0.060305798411764144, 0.06025889110391548, 0.060212049658012114, 0.060165273932447444, 0.06011856378605118, 0.06007191907808777, 0.060025339668254324, 0.059978825416679114, 0.05993237618391971, 0.05988599183096121, 0.059839672219214546, 0.059793417210514635, 0.05974722666711885, 0.05970110045170508, 0.05965503842737021, 0.05960904045762828, 0.05956310640640885, 0.059517236138055374, 0.059471429517323385, 0.05942568640937901, 0.05938000667979709, 0.05933439019455976, 0.059288836820054605, 0.05924334642307315, 0.05919791887080923, 0.05915255403085732, 0.05910725177121095, 0.059062011960261084, 0.05901683446679451, 0.05897171915999236, 0.058926665909428425, 0.05888167458506759, 0.058836745057264286, 0.05879187719676101, 0.05874707087468668, 0.05870232596255512, 0.05865764233226346, 0.05861301985609087, 0.058568458406696625, 0.05852395785711903, 0.05847951808077359, 0.05843513895145173, 0.05839082034331909, 0.0583465621309143, 0.058302364189147304, 0.05825822639329796, 0.05821414861901464, 0.058170130742312656, 0.05812617263957297, 0.05808227418754061, 0.058038435263323265, 0.057994655744390014, 0.05795093550856962, 0.05790727443404944, 0.05786367239937383, 0.05782012928344271, 0.05777664496551035, 0.057733219325183854, 0.05768985224242182, 0.057646543597532934, 0.05760329327117475, 0.057560101144352074, 0.057516967098415896, 0.05747389101506181, 0.05743087277632887, 0.05738791226459811]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid_function(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid_function(x) * (1 - sigmoid_function(x))\n",
    "\n",
    "def summation_unit(inputs, weights):\n",
    "    return np.dot(inputs, weights)\n",
    "\n",
    "def backpropagation(inputs, targets, weights, learning_rate, epochs):\n",
    "    errors = []\n",
    "    for epoch in range(epochs):\n",
    "        total_error = 0\n",
    "        for x, y in zip(inputs, targets):\n",
    "            weighted_sum = summation_unit(x, weights)\n",
    "            prediction = sigmoid_function(weighted_sum)\n",
    "            error = y - prediction\n",
    "            delta = error * sigmoid_derivative(weighted_sum)\n",
    "            weights += learning_rate * delta * x\n",
    "            \n",
    "            total_error += np.mean(np.square(error))\n",
    "        \n",
    "        errors.append(total_error / len(inputs))\n",
    "        \n",
    "        if errors[-1] <= 0.002:\n",
    "            break\n",
    "    \n",
    "    return weights, errors\n",
    "\n",
    "# AND gate data\n",
    "inputs = np.array([[1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])  # Adding bias\n",
    "targets = np.array([0, 0, 0, 1])  # AND gate outputs\n",
    "initial_weights = np.random.rand(3)\n",
    "learning_rate = 0.05\n",
    "epochs = 1000\n",
    "\n",
    "# Train neural network\n",
    "final_weights, errors = backpropagation(inputs, targets, initial_weights, learning_rate, epochs)\n",
    "\n",
    "print(\"Final weights:\", final_weights)\n",
    "print(\"Error list:\", errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eea98cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR Gate - Final weights: [ 0.03444769 -0.06860813 -0.0274801 ]\n",
      "XOR Gate - Number of epochs: 1000\n",
      "XOR Gate - Error list: [2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 1, 3, 1, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "# XOR gate logic inputs and targets\n",
    "inputs_xor = np.array([[1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])  # Adding bias\n",
    "targets_xor = np.array([0, 1, 1, 0])  # XOR gate outputs\n",
    "\n",
    "# Initial weights\n",
    "initial_weights_xor = np.random.rand(3)\n",
    "learning_rate_xor = 0.05\n",
    "\n",
    "# Training perceptron for XOR gate\n",
    "final_weights_xor, num_epochs_xor, error_list_xor = perceptron_training(inputs_xor, targets_xor, initial_weights_xor, learning_rate_xor, step_function)\n",
    "print(\"XOR Gate - Final weights:\", final_weights_xor)\n",
    "print(\"XOR Gate - Number of epochs:\", num_epochs_xor)\n",
    "print(\"XOR Gate - Error list:\", error_list_xor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "021e5bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights for 2 output nodes: [[ 0.18038635 -0.32547581]\n",
      " [-0.08285846  0.07737539]\n",
      " [-0.12455034  0.26209178]]\n",
      "Number of epochs for 2 output nodes: 21\n",
      "Error list for 2 output nodes: [4, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def summation_unit(inputs, weights):\n",
    "    return np.dot(inputs, weights)\n",
    "\n",
    "def step_function(x):\n",
    "    # Step activation function applied element-wise\n",
    "    return np.where(x >= 0, 1, 0)\n",
    "\n",
    "def perceptron_training_with_2_outputs(inputs, targets, weights, learning_rate, activation_function):\n",
    "    epochs = 0\n",
    "    errors = []\n",
    "\n",
    "    while True:\n",
    "        total_error = 0\n",
    "        for i in range(len(inputs)):\n",
    "            weighted_sum = summation_unit(inputs[i], weights)\n",
    "            prediction = activation_function(weighted_sum)\n",
    "            error = targets[i] - prediction\n",
    "            total_error += np.sum(error ** 2)\n",
    "\n",
    "            # Update weights\n",
    "            for j in range(len(weights)):\n",
    "                for k in range(len(weights[j])):\n",
    "                    weights[j][k] += learning_rate * error[k] * inputs[i][j]\n",
    "\n",
    "        errors.append(total_error)\n",
    "        epochs += 1\n",
    "\n",
    "        if total_error <= 0.002 or epochs >= 1000:\n",
    "            break\n",
    "\n",
    "    return weights, epochs, errors\n",
    "\n",
    "# Example use case for AND gate logic with 2 output nodes\n",
    "inputs = np.array([[1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])  # Adding bias\n",
    "targets = np.array([[1, 0], [1, 0], [1, 0], [0, 1]])  # AND gate with 2 output nodes\n",
    "initial_weights = np.random.rand(3, 2)  # Two sets of weights for two outputs\n",
    "learning_rate = 0.05\n",
    "\n",
    "final_weights_2_output, num_epochs_2_output, error_list_2_output = perceptron_training_with_2_outputs(\n",
    "    inputs, targets, initial_weights, learning_rate, step_function)\n",
    "\n",
    "print(\"Final weights for 2 output nodes:\", final_weights_2_output)\n",
    "print(\"Number of epochs for 2 output nodes:\", num_epochs_2_output)\n",
    "print(\"Error list for 2 output nodes:\", error_list_2_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf378e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
